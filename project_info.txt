# Fake News Identification Project by Ejaz Ahmed ( Infosys Internship Intern )

  Github Branch Name :- ejaz 

# IDE Information :-
  This project is developed using Visual Studio Code (VS Code) as the primary Integrated Development Environment (IDE).

# 1)Data Collection info :-

  Path of dataset :- Fake_news_identification_Infosys_Internship_Oct2024/data/fake_news_data.csv

1) Source :- Kaggle  ( Download link below here )

             https://www.kaggle.com/datasets/algord/fake-news?resource=download


2) Number of Data Points and Features :-

   Data Points: The dataset contains 15,438 news entries.
   Features: There are 6 features in the dataset.


3) Explaining Each Features :-

   article_id: A unique number assigned to each article.
   title: The headline of the article.
   description: Description for the news article.
   news_url: The URL linking to the full article.
   source_domain: The website or media outlet where the article was published.
   real: A binary value (0 or 1) that indicates whether the news is real (1) or fake (0).






# 2) Data Exploration and Data Preprocessing

* Data Exploration Steps:-

1) Load the Dataset:
   Read the CSV file using Pandas.

2) Initial Info:
   Show basic info about the dataset and print the first five rows.
   
3) Check for Missing Values:
   Count missing values in each column and total missing entries.

4) Check for Duplicates:
   Count duplicate titles to ensure data quality.

5) Real vs. Fake News Distribution:
   Show how many articles are real and how many are fake.

6) Visualize Distribution:
   Create a bar chart to show the percentage of real vs. fake news.

7) Remaining Data Points:
   Print the count of data points left after cleaning.


 * Data Preprocessing Steps:-

1) Lowercasing: 
   All text in the description column is converted to lowercase.

2) Removing Punctuation and Numbers:
   Any punctuation and numbers are removed from the descriptions.

3) Handling Missing Values:
   Missing descriptions are replaced with an empty string.

4) Checking for Duplicates:
   Duplicates in the titles are identified.

5) Save Processed Data:
   Saves the cleaned dataset as processed_data.csv.


